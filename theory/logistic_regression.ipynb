{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "# L2 regularised Logisitc Regression\n\n*Logistic Regression using maximum likelihood with $L_2$ regularization penalty*\n\n---\n* [Implementation in Python](../pymlalgo/regression/ridge_regression.py)\n* [Demo](../demo/ridge_regression_demo.ipynb)\n\n---\n\n### Features and Labels Shapes\nEach training example, $x_i$ is composed of $d$ features and hence it can be represented using a vector of shape $d \\times 1$  The matrix of all training examples, $\\bf{X}$ is of dimension $d \\times n$ where $n$ is the number of training examples and $d$ is the number of features. As such each training example is a column of the matrix $\\bf{X}$.\n\nEach label $y_i$ is a scalar and there are $n$ labels for each training example. The matrix of labels, $\\bf{Y}$ is of shape $n \\times 1$. \n\nAfter training, weights will be assigned to each feature. The shape of the weight vector $\\beta$ is $d \\times 1$\n\n### Cost Function\n\nThe cost function for logistic regression is given as:\n$$F(\\beta) \u003d \\frac{1}{n} \\sum_{i\u003d1}^n log(1 + exp(-y_i x_{i}^T \\beta)) + \\lambda ||\\beta||_2^2$$\n\nwhere $\\lambda$ is the regularization coefficient (a hyper parameter).\n\nThe goal is to find the value of $\\beta$ for which $F(\\beta)$ is minimum.\n\n### Gradient Derivation\nTo find the gradient, we will differentiate cost function wrt $\\beta$\n$$F(\\beta) \u003d \\frac{1}{n} \\sum_{i\u003d1}^n log(1 + exp(-y_i x_{i}^T \\beta)) + \\lambda ||\\beta||_2^2$$\n\nThe gradient is calculated as,\n\n$$\\frac{\\partial}{\\partial \\beta}F(\\beta) \u003d \\frac{\\partial}{\\partial \\beta}(\\frac{1}{n} \\sum_{i\u003d1}^n log(1 + exp(-y_i x_{i}^T \\beta)) + \\lambda ||\\beta||_2^2)$$\n\nGiven the linearity of differentiation, the above equation can be tackled one by one.\n\n \n\n$$\\frac{\\partial}{\\partial \\beta} log(1 + exp(-y_i x_{i}^T \\beta))$$\n\nAs a reminder, $y_i$ is a scalar $\\in \\{-1,1\\}$ and $x_i$ is a column vector of shape $d \\times 1$\n\n \n\nUsing the chain rule, we write the derivative as\n\n$$\\frac{1}{1 + exp(-y_i x_{i}^T \\beta)}.exp(-y_i x_{i}^T \\beta).\\frac{\\partial}{\\partial \\beta}(-y_i x_{i}^T \\beta)$$\n\nFor the last part of the equation, we use the identity,\n\n$$\\frac{\\partial}{\\partial X} A^TX \u003d \\frac{\\partial}{\\partial X} X^TA \u003d A$$\n\nand rewrite the derivative as\n\n$$\\frac{1}{1 + exp(-y_i x_{i}^T \\beta)}.exp(-y_i x_{i}^T \\beta).(-y_i x_{i})$$\n\n$$\u003d-y_i x_{i}\\frac{exp(-y_i x_{i}^T \\beta)}{1 + exp(-y_i x_{i}^T \\beta)}$$\n\n$$\u003d-y_i x_{i}\\frac{1}{1 + exp(y_i x_{i}^T \\beta)}$$\n\nTo write the objective function, the thing to minimize is the negative log of the likelihood ratio, where the likelihood ratio is given by\n\n$$\\frac{P(y_i \u003d 1 | x_i, \\beta)}{P(y_i \u003d -1 | x_i, \\beta)}$$\n\nTaking the log, we write\n\n$$log(\\frac{P(y_i \u003d 1 | x_i, \\beta)}{P(y_i \u003d -1 | x_i, \\beta)}) \u003d  x_i^T\\beta$$\n\nwhere the RHS is log of likelihood.\n\nSince the events $y_i \u003d 1$ and $y_i \u003d -1$ are mutually exclusive, we can also write it in the logit form.\n\n$$log(\\frac{P(y_i \u003d 1 | x_i, \\beta)}{1 - P(y_i \u003d 1 | x_i, \\beta)}) \u003d  x_i^T\\beta$$\n\n \n\nTaking the inverse, we can write it in the form\n\n$$P(y_i \u003d 1 | x_i, \\beta) \u003d \\frac{1}{1  + exp(-x_i^T\\beta)}$$\n\nwhere $g(z) \u003d \\frac{1}{1 - e^{-z}}$ is called the expit function. Thus we can write\n\n$$P(y_i \u003d 1 | x_i, \\beta) \u003d g(x_i^T\\beta)$$\n\n$$P(y_i \u003d -1 | x_i, \\beta) \u003d 1 - g(x_i^T\\beta) \u003d g(-x_i^T\\beta)$$\n\nHere we used the identity $1 - g(z) \u003d g(-z)$.  \n\nUsing the two equations,  we can write,\n\n$$P(y_i | x_i, \\beta) \u003d g(y_ix_i^T\\beta) \u003d \\frac{1}{1 + exp(-y_ix_i^T\\beta)}$$\n\n \n\nUsing this result and substituting in the derivative, we have\n\n$$\u003d-y_i x_{i}\\frac{1}{1 + exp(y_i x_{i}^T \\beta)}$$\n\n$$\u003d-y_i x_{i}g(-y_ix_i^T\\beta)$$\n\n$$\u003d-y_i x_{i}(1 - P(y_i | x_i, \\beta))$$\n\n \n\nThus,\n\n$$\\frac{\\partial}{\\partial\\beta}F(\\beta) \u003d \\frac{1}{n} \\sum_{i\u003d1}^n -y_i x_{i}(1 - p_i) + 2\\lambda\\beta$$\n\nHere, $p_i \u003d P(y_i | x_i,\\beta)$ \n\nThe final step is to convert it to a matrix form. We will write a matrix $P$ of shape $n \\times n$ where\n\n$$P \u003d I - diag[p_1, p_2, ....., p_n] \u003d diag[1-p_1, 1- p_2,....,1-p_n]$$\n\nThus, in the matrix form, we can write,\n\n$$\\frac{\\partial}{\\partial\\beta}F(\\beta) \u003d -\\frac{1}{n} XYP + 2\\lambda\\beta$$\n\n \n\nSince, $P$ is a diagonal matrix, we can also write it as:\n\n$$\\frac{\\partial}{\\partial\\beta}F(\\beta) \u003d -\\frac{1}{n} XPY + 2\\lambda\\beta$$"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}