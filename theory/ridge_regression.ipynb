{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "\n",
    "*Linear Regression using least square loss with $L_2$ regularization penalty*\n",
    "\n",
    "---\n",
    "* [Implementation in Python](../pymlalgo/regression/ridge_regression.py)\n",
    "* [Demo](../demo/ridge_regression_demo.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "### Features and Labels Shapes\n",
    "Each training example, $x_i$ is composed of $d$ features and hence it can be represented using a vector of shape $d \\times 1$  The matrix of all training examples, $\\bf{X}$ is of dimension $d \\times n$ where $n$ is the number of training examples and $d$ is the number of features. As such each training example is a column of the matrix $\\bf{X}$.\n",
    "\n",
    "Each label $y_i$ is a scalar and there are $n$ labels. The matrix of labels, $\\bf{Y}$ is of shape $n \\times 1$. \n",
    "\n",
    "After training, weights will be assigned to each feature. The shape of the weight vector $\\beta$ is $d \\times 1$\n",
    "\n",
    "### Loss and Cost Function\n",
    "The loss function is given as:\n",
    "$$\\mathcal{L}(\\hat{y}, y) = (y_i - x_i^T\\beta)^2$$\n",
    "\n",
    "The cost function is given as:\n",
    "$$F(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_i^T\\beta)^2 + \\lambda ||\\beta||_2^2$$  \n",
    "\n",
    "where $\\lambda$ is the regularization coefficient (a hyper parameter).\n",
    "\n",
    "The goal is to find the value of $\\beta$ for which $F(\\beta)$ is minimum.\n",
    "\n",
    "### Gradient Derivation\n",
    "\n",
    "For $d = 1$ and $n = 1$:  \n",
    "$$F(\\beta) = \\frac{1}{1}\\sum_{i=1}^{1}(y_i - x_i^T\\beta)^2 + \\lambda ||\\beta||_2^2$$\n",
    "$$F(\\beta) = (y - x\\beta)^2 + \\lambda \\beta^2$$ \n",
    "taking the derivative w.r.t $\\beta$,\n",
    "$$F^{'}(\\beta) = 2(y - x\\beta)(-x) + 2\\lambda \\beta$$\n",
    "$$F^{'}(\\beta) = -2x(y - x\\beta) + 2\\lambda \\beta$$\n",
    "\n",
    "For $d > 1$ and $n > 1$  \n",
    "$$F(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_i^T\\beta)^2 + \\lambda ||\\beta||_2^2$$\n",
    "\n",
    "$L_2$ norm of a matrix $A$ with dimensions $m \\times n$ can be written as,    \n",
    "$$||A||_2 = (\\sum_{i=1}^m \\sum_{j=1}^n |A_{ij}|^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "For any column vector of shape $d \\times 1$\n",
    "$$ v =  \\left[ \\begin{matrix} \n",
    "1 \\\\ 2 \\\\ 3 \\\\ 4 \n",
    "\\end{matrix} \\right]$$\n",
    "$$v^Tv = \\left[ \\begin{matrix} \n",
    "1 \\\\ 2 \\\\ 3 \\\\ 4 \n",
    "\\end{matrix} \\right] \\left[ \\begin{matrix} \n",
    "1 & 2 & 3 & 4 \n",
    "\\end{matrix} \\right] = \\sum_{i=1}^d v_i^2 = ||v||_2^2$$\n",
    "\n",
    "\n",
    "Since, $\\bf{Y} - \\bf{X}^T\\beta$ is of dim $n \\times 1$, $L_2$ norm can be written as\n",
    "$$\\sum_{i=1}^{n}(y_i - x_i^T\\beta)^2 = ||\\bf{Y} - \\bf{X}^T\\beta||_2^2$$\n",
    "\n",
    "$$F(\\beta) = \\frac{1}{n}||\\bf{Y} - \\bf{X}^T\\beta||_2^2 + \\lambda ||\\beta||_2^2$$\n",
    "\n",
    "let's assume the first term and second term to be $g(\\beta)$ and $h(\\beta)$ respectively\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta} h(\\beta) = \\frac{\\partial}{\\partial \\beta} \\lambda ||\\beta||_2^2 = 2\\lambda \\beta$$  \n",
    "\n",
    "  \n",
    "$$\\frac{\\partial}{\\partial \\beta} g(\\beta) = \\frac{\\partial}{\\partial \\beta} ||\\bf{Y} - \\bf{X}^T\\beta||_2^2 = \\frac{\\partial}{\\partial \\beta} (\\bf{Y} - \\bf{X}^T\\beta)^T(\\bf{Y} - \\bf{X}^T\\beta)$$\n",
    "\n",
    "Expanding $g(\\beta)$ and differentiate the terms individually:\n",
    "$$g(\\beta) = \\bf{Y}^T\\bf{Y} - \\bf{Y}^T\\bf{X}^T\\beta - (\\bf{X}^T\\beta)^T\\bf{Y} + (\\bf{X}^T\\beta)^T(\\bf{X}^T\\beta)$$\n",
    "$$=\\bf{Y}^T\\bf{Y} - (\\bf{X}\\bf{Y})^T\\beta - \\beta^T\\bf{X}\\bf{Y} + \\beta^T\\bf{X}\\bf{X}^T\\beta$$\n",
    "Now,\n",
    "$$\\frac{\\partial}{\\partial{\\beta}}\\bf{Y}^T\\bf{Y} =0$$  \n",
    "\n",
    "Using the identity: $\\frac{\\partial}{\\partial{X}} A^TX = \\frac{\\partial}{\\partial{X}} A^TX = A^T$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial{\\beta}}(-(\\bf{X}\\bf{Y})^T\\beta) = -\\bf{XY}$$\n",
    "$$\\frac{\\partial}{\\partial{\\beta}}(-(\\beta^T\\bf{X}\\bf{Y})) = -\\bf{XY}$$\n",
    "\n",
    "Using the identity: $\\frac{\\partial}{\\partial{X}} X^TAX = (A + A^T\n",
    ")X$\n",
    "$$\\frac{\\partial}{\\partial{\\beta}}\\beta^T\\bf{X}\\bf{X}^T\\beta = (\\bf{XX}^T + (\\bf{XX}^T)^T)\\beta$$\n",
    "$$=(\\bf{XX}^T + \\bf{XX}^T)\\beta$$\n",
    "$$=2\\bf{XX}^T\\beta$$\n",
    "\n",
    "Collecting all the 4 terms, and taking mean of all the losses by dividing by $n$\n",
    "$$\\frac{1}{n}(0 -\\bf{XY} - \\bf{XY} + 2\\bf{XX}^T\\beta)$$\n",
    "$$=-\\frac{2\\bf{X}}{n}(\\bf{Y} - \\bf{X}^T\\beta)$$\n",
    "\n",
    "Collecting both $g(\\beta)$ and $h(\\beta)$:  \n",
    "$$\\nabla F(\\beta) = -\\frac{2\\bf{X}}{n}(\\bf{Y} - \\bf{X}^T\\beta)  + 2 \\lambda \\beta$$\n",
    "\n",
    "\n",
    "## Accuracy using R Squared\n",
    "Once the weights have been calculated using gradient descent, the predictions can be made using\n",
    "$$\\hat{\\bf{Y}} = \\bf{X}^T\\beta$$\n",
    "\n",
    "The accuracy of the model is given by the R squared a.k.a [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n",
    "It is calculated as:\n",
    "$$ 1-\\frac{\\text{total squared error}}{\\text{total variation}} = 1 -\\frac{\\sum_{i=1}^n(\\hat{y_i}-y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{\\bf{Y}})^2 }$$\n",
    "\n",
    "The maximum value of R squared is 1 and higher values denote higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
