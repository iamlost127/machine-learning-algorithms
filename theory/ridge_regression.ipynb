{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Ridge Regression\n",
    "\n",
    "*Linear Regression using least square loss with $L_2$ regularization penalty*\n",
    "\n",
    "---\n",
    "* [Implementation in Python](../pymlalgo/regression/ridge_regression.py)\n",
    "* [Demo](../demo/ridge_regression_demo.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "### Symbols and Conventions\n",
    "Refer to [Symbols and Conventions](symbols_and_conventions.ipynb) for details. In summary:\n",
    "* $n$ is the number of training examples\n",
    "* $d$ is the number of features in each training example (a.k.a the dimension of the training example)\n",
    "* $X$ is the features matrix of shape $n \\times d$\n",
    "* $Y$ is the labels matrix of shape $n \\times 1$\n",
    "* $W$ is the weight matrix of shape $d \\times 1$\n",
    "\n",
    "### Loss and Cost Function\n",
    "The loss function is given as:\n",
    "$$\\mathcal{L}(\\hat{y_i}, y_i) = (y_i - x_i W)^2$$\n",
    "\n",
    "The cost function is given as:\n",
    "$$F(W) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_i W)^2 + \\lambda ||W||_2^2$$  \n",
    "\n",
    "where $\\lambda$ is the regularization coefficient (a hyper parameter).\n",
    "\n",
    "The goal is to find the value of $W$ for which $F(W)$ is minimum.\n",
    "\n",
    "### Gradient Derivation\n",
    "\n",
    "For $d = 1$ and $n = 1$:  \n",
    "$$F(W) = \\frac{1}{1}\\sum_{i=1}^{1}(y_i - x_iW)^2 + \\lambda ||W||_2^2$$\n",
    "$$F(W) = (y - xW)^2 + \\lambda W^2$$ \n",
    "taking the derivative w.r.t $W$,\n",
    "$$F^{'}(W) = 2(y - xW)(-x) + 2\\lambda W$$\n",
    "$$F^{'}(W) = -2x(y - xW) + 2\\lambda W$$\n",
    "\n",
    "For $d > 1$ and $n > 1$  \n",
    "$$F(W) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_iW)^2 + \\lambda ||W||_2^2$$\n",
    "\n",
    "$L_2$ norm of a matrix $A$ with dimensions $m \\times n$ can be written as,    \n",
    "$$||A||_2 = (\\sum_{i=1}^m \\sum_{j=1}^n |A_{ij}|^2)^{\\frac{1}{2}}$$\n",
    "\n",
    "For any column vector of shape $d \\times 1$\n",
    "$$ v =  \\left[ \\begin{matrix} \n",
    "1 \\\\ 2 \\\\ 3 \\\\ 4 \n",
    "\\end{matrix} \\right]$$\n",
    "$$v^Tv = \\left[ \\begin{matrix} \n",
    "1 & 2 & 3 & 4 \n",
    "\\end{matrix} \\right] \\left[ \\begin{matrix} \n",
    "1 \\\\ 2 \\\\ 3 \\\\ 4 \n",
    "\\end{matrix} \\right] = \\sum_{i=1}^d v_i^2 = ||v||_2^2$$\n",
    "\n",
    "\n",
    "Since, $Y - XW$ is of dim $n \\times 1$, $L_2$ norm can be written as\n",
    "$$\\sum_{i=1}^{n}(y_i - x_iW)^2 = ||Y - XW||_2^2$$\n",
    "\n",
    "$$F(W) = \\frac{1}{n}||Y - XW||_2^2 + \\lambda ||W||_2^2$$\n",
    "\n",
    "let's assume the first term and second term to be $g(W)$ and $h(W)$ respectively\n",
    "\n",
    "$$\\frac{\\partial}{\\partial W} h(W) = \\frac{\\partial}{\\partial W} \\lambda ||W||_2^2 = 2\\lambda W$$  \n",
    "\n",
    "  \n",
    "$$\\frac{\\partial}{\\partial W} g(W) = \\frac{\\partial}{\\partial W} ||Y - XW||_2^2 = \\frac{\\partial}{\\partial W} (Y - XW)^T(Y - XW)$$\n",
    "\n",
    "Expanding $g(W)$ and differentiate the terms individually:\n",
    "$$g(W) = Y^TY - Y^TXW - (XW)^TY + (XW)^T(XW)$$\n",
    "$$=Y^TY - (X^TY)^TW - W^T(X^TY) + W^TX^TXW$$\n",
    "Now,\n",
    "$$\\frac{\\partial}{\\partial{W}}Y^TY =0$$  \n",
    "\n",
    "Using the identity: $\\frac{\\partial}{\\partial{X}} A^TX = \\frac{\\partial}{\\partial{X}} X^TA = A$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial{W}}(-(X^TY)^TW) = -X^TY$$\n",
    "$$\\frac{\\partial}{\\partial{W}}(-W^T(X^TY)) = -X^TY$$\n",
    "\n",
    "Using the identity: $\\frac{\\partial}{\\partial{X}} X^TAX = (A + A^T\n",
    ")X$\n",
    "$$\\frac{\\partial}{\\partial{W}}W^TX^TXW = (X^TX + (X^TX)^T)W$$\n",
    "$$=(X^TX + X^TX)W$$\n",
    "$$=2 X^TXW$$\n",
    "\n",
    "Collecting all the 4 terms, and taking mean of all the losses by dividing by $n$\n",
    "$$\\frac{1}{n}(0 -X^TY - X^TY + 2X^TXW)$$\n",
    "$$=-\\frac{2X^T}{n}(Y - XW)$$\n",
    "\n",
    "Collecting both $g(W)$ and $h(W)$:  \n",
    "$$\\nabla F(W) = -\\frac{2X^T}{n}(Y - XW)  + 2 \\lambda W$$\n",
    "\n",
    "\n",
    "## Accuracy using R Squared\n",
    "Once the weights have been calculated using gradient descent, the predictions can be made using\n",
    "$$\\hat{Y} = XW$$\n",
    "\n",
    "The accuracy of the model is given by the R squared a.k.a [Coefficient of Determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).\n",
    "It is calculated as:\n",
    "$$ 1-\\frac{\\text{total squared error}}{\\text{total variation}} = 1 -\\frac{\\sum_{i=1}^n(\\hat{y_i}-y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{Y})^2 }$$\n",
    "\n",
    "The maximum value of R squared is 1 and higher values denote higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
